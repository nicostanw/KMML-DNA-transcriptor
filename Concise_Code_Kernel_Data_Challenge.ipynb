{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise code for the Kernel Data Challenege\n",
    "\n",
    "\n",
    "This is a concise code where all the solvers and the kernels we used are implemented without the help of ML library like Scikit Learn. In this code we implement few examples for each kernel and solvers:\n",
    "\n",
    "1. Gaussian Kernel with Kernel Ridge Regression\n",
    "2. Spectrum Kernel with Kernel Logistic Regression\n",
    "3. Mismatch kernel with Kernel SVM\n",
    "\n",
    "The hyperparameters we used for the examples are not the ones we selected for the submissions. \n",
    "In the complete code, we built the three estimators (Ridge, Logistic, SVM) with the solvers of this code, compatible with the scikit learn's API in order to perform efficient cross validation with the GridSearchCV and RandomizedSearchCV functions.\n",
    "This code only gives a few examples to show that our functions work.\n",
    "To completely see the amount of work we have done for this project, please see the complete code. \n",
    "\n",
    "Thank you and have good reading!\n",
    "\n",
    "*PS: We use CVXOPT as QP solvers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "from itertools import product, compress\n",
    "from scipy.linalg import solve, lstsq\n",
    "from itertools import compress, product, combinations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation of the preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0train = pd.read_csv(\"Data/Xtr0_mat100.csv\", sep=' ', header=None).to_numpy()\n",
    "X_1train = pd.read_csv(\"Data/Xtr1_mat100.csv\", sep=' ', header=None).to_numpy()\n",
    "X_2train = pd.read_csv(\"Data/Xtr2_mat100.csv\", sep=' ', header=None).to_numpy()\n",
    "\n",
    "Y_0 = pd.read_csv(\"Data/Ytr0.csv\", sep=',')\n",
    "Y_1 = pd.read_csv(\"Data/Ytr1.csv\", sep=',')\n",
    "Y_2 = pd.read_csv(\"Data/Ytr2.csv\", sep=',')\n",
    "\n",
    "Y_0train = np.where(Y_0[\"Bound\"]==0, -1, Y_0[\"Bound\"])\n",
    "Y_1train = np.where(Y_1[\"Bound\"]==0, -1, Y_1[\"Bound\"])\n",
    "Y_2train = np.where(Y_2[\"Bound\"]==0, -1, Y_2[\"Bound\"])\n",
    "\n",
    "X_0test = pd.read_csv(\"Data/Xte0_mat100.csv\", sep=' ', header=None).to_numpy()\n",
    "X_1test = pd.read_csv(\"Data/Xte1_mat100.csv\", sep=' ', header=None).to_numpy()\n",
    "X_2test = pd.read_csv(\"Data/Xte2_mat100.csv\", sep=' ', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kernel_Ridge_Regression(X_train, y_train, lbd=1, weight=False, gamma=1, degree=2, c0=1, k=2, kernel='rbf'):\n",
    "    if kernel==\"rbf\":\n",
    "        K = rbf_kernel(X_train, gamma)\n",
    "    elif kernel==\"poly\":\n",
    "        K = poly_kernel(X_train, X_train, degree, c0)\n",
    "    elif kernel==\"spectrum\":\n",
    "        K = spectrum_kernel(X_train,k)\n",
    "    elif kernel==\"precomputed\":\n",
    "        K = X_train\n",
    "    n = K.shape[0]\n",
    "    w = weight\n",
    "\n",
    "    if isinstance(weight, bool):\n",
    "        A = (K+n*lbd*np.eye(n))\n",
    "        alpha = solve(A,y_train,assume_a=\"sym\")\n",
    "        return alpha\n",
    "\n",
    "    elif isinstance(weight, str):\n",
    "            w1 = (y_train==1).mean()\n",
    "            w0 = 1-w1\n",
    "            w=np.where(y_train==1, w1, w0)\n",
    "    wi = (1/w)\n",
    "\n",
    "    A = K+n*lbd*wi*np.eye(n)\n",
    "    alpha = solve(A, y_train, assume_a=\"sym\")\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Reweighted Least Square for Kernel Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    return 1 / (1+np.exp(-v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IRLS(X_train, y_train, lbd=1, ga=1, degree=2, c0=1, k=2, ker=\"rbf\", n_iter=15, eps=10**-6, method='slow'):\n",
    "    n = y_train.shape[0]\n",
    "    if ker==\"rbf\":\n",
    "        K = rbf_kernel(X_train, ga)\n",
    "    elif ker==\"poly\":\n",
    "        K = poly_kernel(X_train, X_train, degree, c0)\n",
    "    elif ker==\"spectrum\":\n",
    "        K = spectrum_kernel(X_train, k)\n",
    "    elif ker==\"precomputed\":\n",
    "        K = X_train\n",
    "    # alpha = Kernel_Ridge_Regression(K, y_train, lbd, False, 1, bs, \"precomputed\")\n",
    "    # alpha = np.zeros(n)\n",
    "    # l = []\n",
    "    \n",
    "    alpha=np.zeros(n)\n",
    "    for i in range(n_iter):\n",
    "   \n",
    "        alpha_old = alpha\n",
    "\n",
    "        m = K.dot(alpha)\n",
    "        # l.append(log_loss(y_train*m).mean() + lbd*alpha.dot(m))\n",
    "\n",
    "        p = sigmoid(m)\n",
    "       \n",
    "        weight = p*(1-p)\n",
    "        weight = np.where(weight<0.000001, 0.000001, weight)\n",
    "        \n",
    "        u = np.where(sigmoid(y_train*m)<0.000001, 0.000001, sigmoid(y_train*m))\n",
    "        z = m + y_train/u\n",
    "    \n",
    "        S = np.diag(weight**-1)\n",
    "        A = (K+2*lbd*n*S)\n",
    "        alpha = solve(A, z, assume_a=\"sym\")\n",
    "\n",
    "        # print(np.linalg.norm(alpha_old-alpha))\n",
    "\n",
    "        if np.linalg.norm(alpha_old-alpha) < eps:\n",
    "            break \n",
    "    return alpha # ,l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, y_train, C=1, gamma=1, degree=2, c0=1, k=2, kernel=\"rbf\"):\n",
    "    n = y_train.shape[0]\n",
    "    if kernel==\"rbf\":\n",
    "        K = rbf_kernel(X_train, gamma)\n",
    "    elif kernel==\"poly\":\n",
    "        K = poly_kernel(X_train, X_train, degree, c0)\n",
    "    elif kernel==\"spectrum\":\n",
    "        K = spectrum_kernel(X_train, k)\n",
    "    elif kernel==\"precomputed\":\n",
    "        K = X_train\n",
    "    P = matrix(K, tc='d')\n",
    "    q = matrix(-y_train, tc='d')\n",
    "    g1 = np.diag(y_train)\n",
    "    G = matrix(np.vstack((g1, -g1)), tc='d')\n",
    "    h = matrix(np.hstack((np.repeat(C,n), np.zeros(n))), tc='d')\n",
    "    solvers.options['show_progress'] = False\n",
    "    sol = solvers.qp(P,q,G,h)\n",
    "    return np.array(sol['x']).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions functions\n",
    "For all of estimateurs a test point $x$ is affected to class 1 if $f(x)=\\sum_{i=1}^{n}\\alpha_ik(x,x_i)>0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(alpha, X_test, X_train, kernel, gamma=1, degree=2, c0=1, k=2):\n",
    "        if kernel==\"precomputed\":\n",
    "            # here the matrix k(x_test,x_train) is already computed in X_test\n",
    "            return X_test.dot(alpha)\n",
    "       \n",
    "        elif kernel==\"rbf\":\n",
    "            return K_rbf_kernel(X_test, X_train, gamma).dot(alpha) \n",
    "        elif kernel==\"poly\":\n",
    "            return poly_kernel(X_test, X_train, degree, c0).dot(alpha) \n",
    "        elif kernel==\"spectrum\":\n",
    "            return K_spectrum_kernel(X_test, X_train, k).dot(alpha) \n",
    "\n",
    "def predict(alpha, X_test, X_train, kernel, gamma=1, degree=2, c0=1, k=2):\n",
    "        scores = decision_function(alpha, X_test, X_train, kernel, gamma, degree, c0, k)\n",
    "        indices = (scores > 0).astype(np.int)\n",
    "        return np.array([-1,1])[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example Gaussian kernel with Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X_train, gamma):\n",
    "    X_norm = np.sum(X_train ** 2, axis = -1)\n",
    "    return np.exp(-gamma*(X_norm[:,None]+X_norm[None,:]-2*X_train.dot(X_train.T)))\n",
    "\n",
    "def K_rbf_kernel(X_test, X_train, gamma):\n",
    "    Xtt_norm = np.sum(X_test ** 2, axis = -1)\n",
    "    Xtr_norm = np.sum(X_train ** 2, axis = -1)\n",
    "    return np.exp(-gamma*(Xtt_norm[:,None]+Xtr_norm[None,:]-2*X_test.dot(X_train.T)))\n",
    "\n",
    "def poly_kernel(X, Y, degree, c0):\n",
    "    return (X.dot(Y.T)+c0)**degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some robustness we standardize the training set. Of course the validation set and the test set will be \"standardized\" with respect with the train mean and standard deviation. This also allow us to fix gamma of the gaussian kernel to be 1/dimension of features space which is the \"natural\" value when the features are standardized. This will help when performing cross validation because we will have only one parameter to optimize: the regularization parameter lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardize(X, mean=False, sigma=False):\n",
    "    if isinstance(mean, bool):\n",
    "        mean = X.mean(axis=0)\n",
    "        sigma = X.std(axis=0)\n",
    "        return (X-mean) / sigma, mean, sigma\n",
    "    else:\n",
    "        return (X-mean) / sigma\n",
    "      \n",
    "def split_train_validation_set(X_train, Y_train, train_size=0.8):\n",
    "    n_train = int(0.8*Y_train.shape[0])\n",
    "    idx = np.random.choice(np.arange(Y_train.shape[0]), size=n_train, replace=False)\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        return X_train[idx], Y_train[idx], np.delete(X_train, idx, 0), np.delete(Y_train, idx)\n",
    "    else:\n",
    "        return X_train.iloc[idx], Y_train[idx], X_train.drop(list(idx)), np.delete(Y_train, idx)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0, Ytr0, Xva0, Yva0 = split_train_validation_set(X_0train, Y_0train, train_size=0.8)\n",
    "Xtr0_standardize, mean, sigma = Standardize(Xtr0)\n",
    "\n",
    "Xva0_standardize = Standardize(Xva0, mean, sigma)\n",
    "d = 1/X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Kernel Ridge Regression on this validation set for lambda=1, gaussian kernel with parameter gamma=0.333\n",
      " is 0.5700%\n"
     ]
    }
   ],
   "source": [
    "lam = 1\n",
    "alpha = Kernel_Ridge_Regression(Xtr0_standardize, Ytr0, lbd=lam, gamma=d, kernel='rbf')\n",
    "\n",
    "predictions = predict(alpha, Xva0_standardize, Xtr0_standardize, \"rbf\", gamma=1/d)\n",
    "\n",
    "accuracy = (predictions==Yva0).mean()\n",
    "print(\"The accuracy for Kernel Ridge Regression on this validation set for lambda={}, gaussian kernel with parameter gamma={:.3f}\\n is {:.4f}%\".format(lam,d,accuracy))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the real data for string kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0train=pd.read_csv('Data/Xtr0.csv', sep=',')['seq']\n",
    "X_1train=pd.read_csv('Data/Xtr1.csv', sep=',')['seq']\n",
    "X_2train=pd.read_csv('Data/Xtr2.csv', sep=',')['seq']\n",
    "\n",
    "X_0test=pd.read_csv('Data/Xte0.csv', sep=',')['seq']\n",
    "X_1test=pd.read_csv('Data/Xte1.csv', sep=',')['seq']\n",
    "X_2test=pd.read_csv('Data/Xte2.csv', sep=',')['seq']\n",
    "\n",
    "Y_0=pd.read_csv(\"Data/Ytr0.csv\",sep=',')\n",
    "Y_1=pd.read_csv(\"Data/Ytr1.csv\",sep=',')\n",
    "Y_2=pd.read_csv(\"Data/Ytr2.csv\",sep=',')\n",
    "\n",
    "Y_0train=np.where(Y_0[\"Bound\"]==0,-1,Y_0[\"Bound\"])\n",
    "Y_1train=np.where(Y_1[\"Bound\"]==0,-1,Y_1[\"Bound\"])\n",
    "Y_2train=np.where(Y_2[\"Bound\"]==0,-1,Y_2[\"Bound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Examples Spectrum Kernel with Kernel Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dic_voc(k):\n",
    "    return {''.join(v):i for i,v in enumerate(product(\"ACGT\",repeat=k))}\n",
    "    \n",
    "\n",
    "def phi_X(X,voc):\n",
    "    x=np.zeros(len(voc))\n",
    "    for u in X:\n",
    "        x[voc[u]]+=1\n",
    "    return x\n",
    "def spectrum_kernel(X,k):\n",
    "    voc=build_dic_voc(k)\n",
    "    phi_x=np.vstack(X.apply(lambda x: [x[i:i+k] for i in range(0, len(x)-k+1)]).apply(phi_X,args=(voc,)).to_numpy())\n",
    "    return (phi_x.dot(phi_x.T))\n",
    "\n",
    "\n",
    "def K_spectrum_kernel(X,Y,k):\n",
    "    voc=build_dic_voc(k)\n",
    "    phi_x=np.vstack(X.apply(lambda x: [x[i:i+k] for i in range(0, len(x)-k+1)]).apply(phi_X,args=(voc,)).to_numpy())\n",
    "    phi_y=np.vstack(Y.apply(lambda x: [x[i:i+k] for i in range(0, len(x)-k+1)]).apply(phi_X,args=(voc,)).to_numpy())\n",
    "    return phi_x.dot(phi_y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0,Ytr0,Xva0,Yva0=split_train_validation_set(X_0train,Y_0train,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Kernel Logistic Regression, on this validation set for lambda=1, spectrum kernel with parameter k=8\n",
      " is 0.6075%\n"
     ]
    }
   ],
   "source": [
    "lam=1\n",
    "k=8\n",
    "alpha=IRLS(Xtr0,Ytr0,lbd=lam,k=8,ker='spectrum')\n",
    "\n",
    "predictions=predict(alpha,Xva0,Xtr0,\"spectrum\",k=9)\n",
    "\n",
    "accuracy=(predictions==Yva0).mean()\n",
    "print(\"The accuracy for Kernel Logistic Regression, on this validation set for lambda={}, spectrum kernel with parameter k={}\\n is {:.4f}%\".format(lam,k,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatch Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_mismatch_X(X,k,m,voc):\n",
    "    voc1=[\"A\",\"C\",\"G\",\"T\"]\n",
    "    phi_x=np.zeros(len(voc))\n",
    "    for d in range(0,len(X)-k+1):\n",
    "        x=list(X[d:d+k])\n",
    "        \n",
    "   \n",
    "        for mm in (range(m,-1,-1)):\n",
    "            for i in combinations(range(k),mm):\n",
    "                list_voc_left=[]\n",
    "                for y in i:\n",
    "                    voc_copy=voc1[:]\n",
    "\n",
    "                    voc_copy.remove(x[y])\n",
    "                    list_voc_left.append(voc_copy)\n",
    "\n",
    "                for letters in product(*list_voc_left):\n",
    "                    x_copy=x[:]\n",
    "                    for o,p in enumerate(i):\n",
    "                         x_copy[p]=letters[o]\n",
    "            \n",
    "                    yes=''.join(x_copy)\n",
    "                    \n",
    "                  \n",
    "                    phi_x[voc[yes]]= phi_x[voc[yes]]+1\n",
    "                   \n",
    "    return phi_x\n",
    "            \n",
    "def mismatch_kernel(X,k,m):\n",
    "    \n",
    "    voc=build_dic_voc(k)\n",
    "    phi_x=np.vstack(X.apply(phi_mismatch_X,args=(k,m,voc)).to_numpy())\n",
    "    return (phi_x.dot(phi_x.T))\n",
    "\n",
    "def K_mismatch_kernel(X,Y,k,m):\n",
    "    \n",
    "    voc=build_dic_voc(k)\n",
    "    phi_x=np.vstack(X.apply(phi_mismatch_X,args=(k,m,voc)).to_numpy())\n",
    "    phi_y=np.vstack(Y.apply(phi_mismatch_X,args=(k,m,voc)).to_numpy())\n",
    "    return (phi_x.dot(phi_y.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Kernel SVM, on this validation set for C=1, mismatch kernel with parameter k=9\n",
      " and m=1 is 0.6250%\n"
     ]
    }
   ],
   "source": [
    "C=1\n",
    "k=9\n",
    "m=1\n",
    "K_train_train=mismatch_kernel(Xtr0,k,m)\n",
    "alpha=SVM(K_train_train,Ytr0,C=1,kernel='precomputed')\n",
    "K_val_train=K_mismatch_kernel(Xva0,Xtr0,9,1)\n",
    "\n",
    "predictions=predict(alpha,K_val_train,Xtr0,\"precomputed\")\n",
    "\n",
    "accuracy=(predictions==Yva0).mean()\n",
    "print(\"The accuracy for Kernel SVM, on this validation set for C={}, mismatch kernel with parameter k={}\\n and m={} is {:.4f}%\".format(lam,k,m,accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
